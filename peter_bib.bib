@article{Hen15,
  author = {{Hennig}, P.},
  journal = {SIAM J on Optimization},
  title = {{Probabilistic Interpretation of Linear Solvers}},
  year = {2015},
  volume = {25},
  issue = {1},
}

@article{PN15,
  author = {Hennig, Philipp and Osborne, Michael A. and Girolami, Mark},
  title = {Probabilistic numerics and uncertainty in computations},
  volume = {471},
  number = {2179},
  year = {2015},
  publisher = {The Royal Society},
  journal = {Proceedings of the Royal Society of London A: Mathematical, Physical and Engineering Sciences},
}

@article{Davis:2011,
	author = {Davis, Timothy A. and Hu, Yifan},
	title = {The {University of Florida} Sparse Matrix Collection},
	journal = {ACM Trans. Math. Softw.},
	volume = {38},
	number = {1},
	year = {2011}
} 


@article{Briol2019PI,
abstract = {A research frontier has emerged in scientific computation, wherein numerical error is regarded as a source of epistemic uncertainty that can be modelled. This raises several statistical challenges, including the design of statistical methods that enable the coherent propagation of probabilities through a (possibly deterministic) computational work-flow. This paper examines the case for probabilistic numerical methods in routine statistical computation. Our focus is on numerical integration, where a probabilistic integrator is equipped with a full distribution over its output that reflects the presence of an unknown numerical error. Our main technical contribution is to establish, for the first time, rates of posterior contraction for these methods. These show that probabilistic integrators can in principle enjoy the "best of both worlds", leveraging the sampling efficiency of Monte Carlo methods whilst providing a principled route to assess the impact of numerical error on scientific conclusions. Several substantial applications are provided for illustration and critical evaluation, including examples from statistical modelling, computer graphics and a computer model for an oil reservoir.},
author = {Briol, F-X. and Oates, C. J. and Girolami, M. and Osborne, M. A. and Sejdinovic, D.},
file = {:home/francois/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Briol et al. - 2015 - Probabilistic integration A role in statistical computation.pdf:pdf},
journal = {Statistical Science},
number = {1},
pages = {1--22},
title = {{Probabilistic integration: A role in statistical computation? (with discussion)}},
volume = {34},
year = {2019}
}



@article{Cockayne2018Conjugate,
abstract = {A fundamental task in numerical computation is the solution of large linear systems. The conjugate gradient method is an iterative method which offers rapid convergence to the solution, particularly when an effective preconditioner is employed. However, for more challenging systems a substantial error can be present even after many iterations have been performed. The estimates obtained in this case are of little value unless further information can be provided about the numerical error. In this paper we propose a novel statistical model for this numerical error set in a Bayesian framework. Our approach is a strict generalisation of the conjugate gradient method, which is recovered as the posterior mean for a particular choice of prior. The estimates obtained are analysed with Krylov subspace methods and a contraction result for the posterior is presented. The method is then analysed in a simulation study as well as being applied to a challenging problem in medical imaging.},
author = {Cockayne, J. and Oates, C. J. and Girolami, M.},
file = {:home/francois/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cockayne, Oates, Girolami - 2018 - A Bayesian conjugate gradient method.pdf:pdf},
journal = {arXiv:1801.05242},
title = {{A Bayesian conjugate gradient method}},
year = {2018}
}


@article{OHagan1991,
author = {O'Hagan, A},
journal = {Journal of Statistical Planning and Inference},
keywords = {bayesian},
pages = {245--260},
title = {{Bayes–Hermite quadrature}},
volume = {29},
year = {1991}
}

@book{Rasmussen2006,
author = {Rasmussen, C. and Williams, C.},
file = {:home/francois/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rasmussen, Williams - 2006 - Gaussian Processes for Machine Learning.pdf:pdf},
publisher = {MIT Press},
title = {{Gaussian Processes for Machine Learning}},
year = {2006}
}


@article{Cockayne2019,
abstract = {A fundamental task in numerical computation is the solution of large linear systems. The conjugate gradient method is an iterative method which offers rapid convergence to the solution, particularly when an effective preconditioner is employed. However, for more challenging systems a substantial error can be present even after many iterations have been performed. The estimates obtained in this case are of little value unless further information can be provided about the numerical error. In this paper we propose a novel statistical model for this numerical error set in a Bayesian framework. Our approach is a strict generalisation of the conjugate gradient method, which is recovered as the posterior mean for a particular choice of prior. The estimates obtained are analysed with Krylov subspace methods and a contraction result for the posterior is presented. The method is then analysed in a simulation study as well as being applied to a challenging problem in medical imaging.},
author = {Cockayne, J. and Oates, C. J. and Ipsen, I. C. F. and Girolami, M.},
file = {:home/francois/Dropbox/Work/Papers/Probabilistic Numerics/(2019, Cockayne) A Bayesian Conjugate Gradient Method.pdf:pdf},
journal = {Bayesian Analysis},
keywords = {62c10,62f15,65f10,Krylov sub,krylov subspaces,linear systems,msc 2010 subject classifications,probabilistic numerics},
title = {{A Bayesian Conjugate Gradient Method}},
year = {2019}
}



@incollection{Dashti2017,
author = {Dashti, M. and Stuart, A. M.},
booktitle = {Handbook of Uncertainty Quantification},
file = {:home/francois/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dashti, Stuart, Jul - Unknown - The Bayesian Approach to Inverse Prob- lems.pdf:pdf},
keywords = {bayesian inversion,differential equations,erc and onr for,es-,financial support which led,inverse problems,langevin stochastic partial,markov chain monte carlo,sequential monte carlo,the authors are grateful,tikhonov regularization and map,timators,to epsrc,to the},
pages = {311--428},
publisher = {Springer},
title = {{The Bayesian approach to inverse problems}},
year = {2017}
}



@inproceedings{Sun2007,
abstract = {Objective Bayesian inference for the multivariate normal distribution is illustrated, using different types of formal objective priors (Jeffreys, invari- ant, reference and matching), different modes of inference (Bayesian and frequentist), and different criteria involved in selecting optimal objective priors (ease of computation, frequentist performance, marginalization paradoxes, and decision-theoretic evaluation). In the course of the investigation of the bivariate normal model in Berger and Sun (2006), a variety of surprising results were found, including the availability of objective priors that yield exact frequentist inferences for many functions of the bivariate normal parameters, such as the correlation coefficient. Certain of these results are generalized to the multivariate normal situation. The prior that most frequently yields exact frequentist inference is the right- Haar prior, which unfortunately is not unique. Two natural proposals are studied for dealing with this non-uniqueness: first, mixing over the right-Haar priors; second, choosing the ‘empirical Bayes' right-Haar prior, that which maximizes the marginal likelihood of the data. Quite surprisingly, we show that neither of these possibilities yields a good solution. This is disturbing and sobering. It is yet another indication that improper priors do not behave as do proper priors, and that it can be dangerous to apply ‘understandings' from the world of proper priors to the world of improper priors.},
author = {Sun, D. and Berger, J. O.},
booktitle = {Bayesian Statistics 8},
file = {:home/francois/Dropbox/Work/Papers/Bayesian Statistics/Objective Bayes/(2006, Sun {\&} Berger) Objective priors for the multivariate normal model.pdf:pdf},
keywords = {Jeffreys prior,Kullback-Leibler divergence,invariant priors,matching priors,multivariate normal distribution,reference priors},
pages = {525--554},
title = {{Objective priors for the multivariate Normal model}},
year = {2007}
}

@article{Alvarez2011Review,
abstract = {Computer Science and Artificial Intelligence Laboratory Technical Report},
author = {{\'{A}}lvarez, M. A. and Rosasco, L. and Lawrence, N. D.},
file = {:home/francois/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/{\'{A}}lvarez, Rosasco, Lawrence - 2012 - Kernels for vector-valued functions A review.pdf:pdf},
journal = {Foundations and Trends in Machine Learning},
number = {3},
pages = {195--266},
title = {{Kernels for vector-valued functions: A review}},
volume = {4},
year = {2012}
}


@article{Chkrebtii2016,
abstract = {We explore the use of probability models for uncertainty arising from discretization of system states defined implicitly by ordinary or partial differential equations. Accounting for this uncertainty is vital for characterising potential inferential bias incurred when likelihoods are constructed based on a numerical approximation over a finite discretization grid. A formalism for inferring fixed but a priori unknown model trajectories is proposed within the forward problem through Bayesian updating of sequentially refined prior models conditional on model interrogations. A one-step-ahead sampling scheme for interrogating the model is studied in detail, its consistency and first order convergence properties are proved. The proposed approach is demonstrated to capture the functional structure and magnitude of the discretization error, while attaining computational scaling of the same order as traditional first order numerical methods, and providing a formal statistical trade-off between accuracy and discretization grid size. Examples illustrate the flexibility of this framework to deal with a wide variety of models that include initial value, delay, and boundary value ordinary differential equations, as well as partial differential equations. Finally, the discussion outlines a wide range of emerging research themes in the new field of probabilistic numerics that naturally follow from the work presented.},
author = {Chkrebtii, O. A. and Campbell, D. A. and Calderhead, B. and Girolami, M.},
file = {:home/francois/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chkrebtii et al. - 2016 - Bayesian solution uncertainty quantification for differential equations.pdf:pdf},
journal = {Bayesian Analysis},
keywords = {bayesian inference,bayesian numerical analysis,dy-,gaussian process regression,namic systems,uncertainty quantification},
number = {4},
pages = {1239--1267},
title = {{Bayesian solution uncertainty quantification for differential equations (with discussion)}},
volume = {11},
year = {2016}
}




@article{Cockayne2016,
abstract = {This paper develops a class of meshless methods that are well-suited to statistical inverse problems involving partial differential equations (PDEs). The methods discussed in this paper view the forcing term in the PDE as a random field that induces a probability distribution over the residual error of a symmetric collocation method. This construction enables the solution of challenging inverse problems while accounting, in a rigorous way, for the impact of the discretisation of the forward problem. In particular, this confers robustness to failure of meshless methods, with statistical inferences driven to be more conservative in the presence of significant solver error. In addition, (i) a principled learning-theoretic approach to minimise the impact of solver error is developed, and (ii) the challenging setting of inverse problems with a non-linear forward model is considered. The method is applied to parameter inference problems in which non-negligible solver error must be accounted for in order to draw valid statistical conclusions.},
author = {Cockayne, J. and Oates, C. J. and Sullivan, T. and Girolami, M.},
file = {:home/francois/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cockayne et al. - 2016 - Probabilistic meshless methods for partial differential equations and Bayesian inverse problems.pdf:pdf},
journal = {arXiv:1605.07811},
title = {{Probabilistic meshless methods for partial differential equations and Bayesian inverse problems}},
year = {2016}
}



@inproceedings{Xi2018MultiOutput,
abstract = {Bayesian probabilistic numerical methods are a set of tools providing posterior distributions on the output of numerical methods. The use of these methods is usually motivated by the fact that they can represent our uncertainty due to incomplete/finite information about the continuous mathematical problem being approximated. In this paper, we demonstrate that this paradigm can provide additional advantages, such as the possibility of transferring information between several numerical methods. This allows users to represent uncertainty in a more faithfully manner and, as a by-product, provide increased numerical efficiency. We propose the first such numerical method by extending the well-known Bayesian quadrature algorithm to the case where we are interested in computing the integral of several related functions. We then demonstrate its efficiency in the context of multi-fidelity models for complex engineering systems, as well as a problem of global illumination in computer graphics.},
author = {Xi, X. and Briol, F-X. and Girolami, M.},
booktitle = {International Conference on Machine Learning, PMLR 80},
file = {:home/francois/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Xi, Briol, Girolami - 2018 - Bayesian quadrature for multiple related integrals.pdf:pdf},
pages = {5369--5378},
title = {{Bayesian quadrature for multiple related integrals}},
year = {2018}
}



@article{DeRoos2017,
abstract = {Solving symmetric positive definite linear problems is a fundamental computational task in machine learning. The exact solution, famously, is cubicly expensive in the size of the matrix. To alleviate this problem, several linear-time approximations, such as spectral and inducing-point methods, have been suggested and are now in wide use. These are low-rank approximations that choose the low-rank space a priori and do not refine it over time. While this allows linear cost in the data-set size, it also causes a finite, uncorrected approximation error. Authors from numerical linear algebra have explored ways to iteratively refine such low-rank approximations, at a cost of a small number of matrix-vector multiplications. This idea is particularly interesting in the many situations in machine learning where one has to solve a sequence of related symmetric positive definite linear problems. From the machine learning perspective, such deflation methods can be interpreted as transfer learning of a low-rank approximation across a time-series of numerical tasks. We study the use of such methods for our field. Our empirical results show that, on regression and classification problems of intermediate size, this approach can interpolate between low computational cost and numerical precision.},
author = {de Roos, F. and Hennig, P.},
file = {:home/francois/Dropbox/Work/Papers/Numerical Analysis/Linear algebra/Linear Systems/(2017, de Roos {\&} Hennig) Krylov subspace recycling for fast-iterative least-squares in machine learning.pdf:pdf},
journal = {arXiv:1706.00241},
title = {{Krylov subspace recycling for fast iterative least-squares in machine learning}},
year = {2017}
}

